{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muhamad Nahrowi Tesis, IndoBERT - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pra-Pemrosesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 788)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import emoji\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset_politik_indonesia.csv')\n",
    "\n",
    "# 1. Bersihkan Teks\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Case folding\n",
    "    text = re.sub(r'http\\S+', '', text)  # Hapus URL\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Hapus karakter non-alfabet kecuali emotikon\n",
    "    return text\n",
    "\n",
    "df['cleaned_comment'] = df['comment'].apply(clean_text)\n",
    "\n",
    "# 2. Label Encoding\n",
    "df['label'] = df['sarcasm'].astype(int)\n",
    "df['has_emoticon'] = df['emoticon'].astype(int)\n",
    "\n",
    "# 3. Ekstraksi Fitur dengan IndoBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=50)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "df['bert_embedding'] = df['cleaned_comment'].apply(get_bert_embedding)\n",
    "\n",
    "# 4. Ekstraksi Fitur Emotikon\n",
    "def extract_emoticons(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "df['emoticons'] = df['comment'].apply(extract_emoticons)\n",
    "\n",
    "# 5. Representasi Emotikon\n",
    "# Untuk representasi emotikon, kita dapat menggunakan pendekatan one-hot encoding atau embedding khusus emotikon.\n",
    "# Sebagai contoh, berikut adalah implementasi sederhana dengan one-hot encoding:\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "emoticon_features = mlb.fit_transform(df['emoticons'])\n",
    "\n",
    "# Gabungkan fitur IndoBERT dan fitur emotikon\n",
    "import numpy as np\n",
    "\n",
    "# Pastikan panjang vektor bert_embedding konsisten\n",
    "bert_embeddings = np.stack(df['bert_embedding'].values)\n",
    "\n",
    "# Gabungkan fitur\n",
    "combined_features = np.hstack((bert_embeddings, emoticon_features))\n",
    "\n",
    "# Dataset final siap untuk pelatihan model\n",
    "print(combined_features.shape)  # Pastikan dimensinya sesuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.1800, Accuracy: 98.19%, Val Loss: 0.0008, Val Accuracy: 100.00%\n",
      "Epoch [2/50], Loss: 0.0008, Accuracy: 100.00%, Val Loss: 0.0002, Val Accuracy: 100.00%\n",
      "Epoch [3/50], Loss: 0.0004, Accuracy: 100.00%, Val Loss: 0.0001, Val Accuracy: 100.00%\n",
      "Epoch [4/50], Loss: 0.0003, Accuracy: 100.00%, Val Loss: 0.0001, Val Accuracy: 100.00%\n",
      "Epoch [5/50], Loss: 0.0002, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [6/50], Loss: 0.0001, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [7/50], Loss: 0.0001, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [8/50], Loss: 0.0001, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [9/50], Loss: 0.0001, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [10/50], Loss: 0.0001, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [11/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [12/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [13/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [14/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [15/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [16/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [17/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [18/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [19/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [20/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [21/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [22/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [23/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [24/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [25/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [26/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [27/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [28/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [29/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [30/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [31/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [32/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [33/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [34/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [35/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [36/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [37/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [38/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [39/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [40/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [41/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [42/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [43/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [44/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [45/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [46/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [47/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [48/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [49/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
      "Epoch [50/50], Loss: 0.0000, Accuracy: 100.00%, Val Loss: 0.0000, Val Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "\n",
    "df['combined_features'] = list(combined_features)\n",
    "\n",
    "# Prepare Data for Training\n",
    "X = torch.tensor(np.array(df['combined_features'].tolist()), dtype=torch.float32)\n",
    "y = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define LSTM Model\n",
    "# class SarcasmClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "#         super(SarcasmClassifier, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initialize hidden state\n",
    "#         c0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initialize cell state\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.dropout(out[:, -1, :])\n",
    "#         out = self.fc(out)\n",
    "#         return out\n",
    "\n",
    "class SarcasmClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(SarcasmClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pastikan input memiliki dimensi (batch_size, sequence_length, input_size)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # Tambahkan dimensi untuk sequence_length jika tidak ada\n",
    "\n",
    "        # Inisialisasi hidden state dan cell state sesuai dengan batch size\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))  # LSTM expects input of (batch_size, seq_len, input_size)\n",
    "        out = self.dropout(out[:, -1, :])  # Ambil output dari timestep terakhir\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "dropout_prob = 0.5\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# cpu\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "model = SarcasmClassifier(input_size, hidden_size, num_layers, num_classes, dropout_prob).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and Validation Loop with Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "patience, trials = 5, 0  # Early stopping criteria\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, '\n",
    "          f'Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trials = 0\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
